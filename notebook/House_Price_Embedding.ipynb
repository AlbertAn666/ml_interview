{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34b222e7-8b77-4849-be48-f61446aeaa1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.8667643 , -0.01246836, -0.21289571, ..., -0.09274033,\n",
       "         -0.13341669,  1.65006527],\n",
       "        [ 0.07410996, -0.50235683, -0.26524463, ..., -0.09274033,\n",
       "         -0.5080097 ,  0.89367742],\n",
       "        [-0.63154574, -0.14607431, -0.17784146, ..., -0.09274033,\n",
       "         -0.5080097 ,  0.13728958],\n",
       "        ...,\n",
       "        [-0.8667643 , -0.45782152, -0.23409563, ..., -0.09274033,\n",
       "         -0.88260272, -1.37548612],\n",
       "        [-0.16110861, -0.6804981 , -0.28337613, ..., -0.09274033,\n",
       "         -0.13341669, -0.61909827],\n",
       "        [ 1.48542135, -0.76956873, -0.65139925, ..., -0.09274033,\n",
       "         -0.13341669,  0.89367742]], shape=(1168, 36)),\n",
       " array([[-0.8667643 , -0.01246836, -0.21159396, ..., -0.09274033,\n",
       "         -1.63178875, -1.37548612],\n",
       "        [ 0.07410996,  1.23452047,  0.14564323, ..., -0.09274033,\n",
       "         -0.88260272,  1.65006527],\n",
       "        [-0.63154574, -0.63596278, -0.16082574, ..., -0.09274033,\n",
       "         -1.25719573,  1.65006527],\n",
       "        ...,\n",
       "        [ 0.07410996, -0.32421557, -0.23158511, ..., -0.09274033,\n",
       "          1.36495537,  0.13728958],\n",
       "        [ 0.30932853, -0.45782152, -0.14929596, ..., -0.09274033,\n",
       "          1.36495537,  0.89367742],\n",
       "        [-0.8667643 , -0.01246836, -0.2389307 , ..., -0.09274033,\n",
       "          0.24117632,  0.89367742]], shape=(292, 36)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "y = df_train[\"SalePrice\"]\n",
    "y_log = np.log1p(y)\n",
    "x = df_train.drop(columns = [\"SalePrice\", \"Id\"])\n",
    "x_test = df_test.drop(columns = [\"Id\"])\n",
    "\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(\n",
    "    x, y_log, test_size = 0.2, random_state = 42)\n",
    "\n",
    "num_features = x.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_features = x.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "num_imputer = SimpleImputer(strategy = \"median\")\n",
    "num_scaler = StandardScaler()\n",
    "\n",
    "Xtr_num = num_imputer.fit_transform(x_train[num_features])\n",
    "Xtr_num = num_scaler.fit_transform(Xtr_num)\n",
    "\n",
    "Xva_num = num_imputer.transform(x_eval[num_features])\n",
    "Xva_num = num_scaler.transform(Xva_num)\n",
    "\n",
    "Xtr_num, Xva_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "378dee92-99d2-4c40-b6e9-2edd08568b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]], shape=(1168, 43)),\n",
       " array([[1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [2, 1, 3, ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 1, 1, 1]], shape=(292, 43)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def build_category_maps(df: pd.DataFrame, cat_cols: List[str]) -> Dict[str, Dict[str, int]]:\n",
    "    maps = {}\n",
    "    for c in cat_cols:\n",
    "        s = df[c].astype(\"string\").fillna(\"__nan__\")\n",
    "        uniq = pd.unique(s)\n",
    "        maps[c] = {v: i+1 for i, v in enumerate(uniq)}\n",
    "    return maps\n",
    "\n",
    "def encode_categories(df: pd.DataFrame, cat_cols: List[str], maps: Dict[str, Dict[str, int]]) -> np.ndarray:\n",
    "    out = np.zeros((len(df), len(cat_cols)), dtype = np.int64)\n",
    "    for j, c in enumerate(cat_cols):\n",
    "        s = df[c].astype(\"string\").fillna(\"__nan__\")\n",
    "        m = maps[c]\n",
    "        out[:, j] = s.map(lambda x: m.get(x, 0)).to_numpy(dtype=np.int64)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "cat_maps = build_category_maps(x_train, cat_features)\n",
    "Xtr_cat = encode_categories(x_train, cat_features, cat_maps)\n",
    "Xva_cat = encode_categories(x_eval, cat_features, cat_maps)\n",
    "\n",
    "Xtr_cat, Xva_cat\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6ade1ac-4135-4495-95a1-66f34bcd6c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train unknown rate: 0.0\n",
      "val unknown rate: 0.00015928639694170118\n",
      "num_features: 36 cat_features: 43\n",
      "cat cardinalities max: 25\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "Xtr_num_t = torch.tensor(Xtr_num, dtype=torch.float32)\n",
    "Xva_num_t = torch.tensor(Xva_num, dtype=torch.float32)\n",
    "\n",
    "Xtr_cat_t = torch.tensor(Xtr_cat, dtype=torch.long)\n",
    "Xva_cat_t = torch.tensor(Xva_cat, dtype=torch.long)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "ytr_n = y_scaler.fit_transform(y_train.to_numpy().reshape(-1,1)).astype(np.float32)\n",
    "yva_n = y_scaler.transform(y_eval.to_numpy().reshape(-1,1)).astype(np.float32)\n",
    "\n",
    "ytr_t = torch.tensor(ytr_n)\n",
    "yva_t = torch.tensor(yva_n)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(Xtr_num_t, Xtr_cat_t, ytr_t), batch_size=32, shuffle=True)\n",
    "\n",
    "print(\"train unknown rate:\", (Xtr_cat == 0).mean())\n",
    "print(\"val unknown rate:\", (Xva_cat == 0).mean())\n",
    "\n",
    "print(\"num_features:\", len(num_features), \"cat_features:\", len(cat_features))\n",
    "print(\"cat cardinalities max:\", max(len(cat_maps[c]) for c in cat_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6fe7d1a-c0a6-47b5-a0bb-99168355308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "def default_emb_dim(n_cat: int) -> int:\n",
    "    return int(min(50, round(math.sqrt(n_cat))))\n",
    "\n",
    "class EmbeddingNN(nn.Module):\n",
    "    def __init__(self, num_dim: int, cat_cardinalities: List[int]):\n",
    "        super().__init__()\n",
    "        self.emb_layers = nn.ModuleList()\n",
    "        emb_dims = []\n",
    "        for n in cat_cardinalities:\n",
    "            d = default_emb_dim(n)\n",
    "            emb_dims.append(d)\n",
    "            self.emb_layers.append(nn.Embedding(num_embeddings=n+1, embedding_dim=d))\n",
    "\n",
    "        self.emb_out_dim = sum(emb_dims)\n",
    "        in_dim = num_dim + self.emb_out_dim\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        embs = []\n",
    "        for i, emb in enumerate(self.emb_layers):\n",
    "            embs.append(emb(x_cat[:, i]))\n",
    "        x = torch.cat([x_num] + embs, dim=1)\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "cat_cardinalities = [len(cat_maps[c]) for c in cat_features]\n",
    "model = EmbeddingNN(num_dim=Xtr_num.shape[1], cat_cardinalities=cat_cardinalities)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1882fc6c-f13f-4139-85e0-b4ad0ccdf16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | val RMSE: 0.35505 | val LOGRMSE: 0.13863 | best: 0.13863 | lr: 3.13e-05 | pat_left: 15\n",
      "Epoch 010 | val RMSE: 0.35588 | val LOGRMSE: 0.13895 | best: 0.13863 | lr: 7.81e-06 | pat_left: 6\n",
      "Early stopped at epoch 16, best=0.13863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13862694799900055"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmse_t(y_true, y_pred):\n",
    "    return torch.sqrt(torch.mean((y_true - y_pred) ** 2)).item()\n",
    "\n",
    "def logrmse_from_norm(y_true_n_t, y_pred_n_t):\n",
    "    y_true_log = y_scaler.inverse_transform(y_true_n_t.detach().cpu().numpy())\n",
    "    y_pred_log = y_scaler.inverse_transform(y_pred_n_t.detach().cpu().numpy())\n",
    "    return float(np.sqrt(np.mean((y_true_log - y_pred_log) ** 2)))\n",
    "\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "patience = 15\n",
    "pat_left = patience\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    model.train()\n",
    "    for xb_num, xb_cat, yb in train_loader:\n",
    "        pred = model(xb_num, xb_cat)\n",
    "        loss = loss_fn(pred, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(Xva_num_t, Xva_cat_t)\n",
    "        val_rmse = rmse_t(yva_t, val_pred)\n",
    "        val_rmse_from_norm = logrmse_from_norm(yva_t, val_pred)\n",
    "\n",
    "    scheduler.step(val_rmse_from_norm)\n",
    "    lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "    if val_rmse_from_norm < best_val - 1e-5:\n",
    "        best_val = val_rmse_from_norm\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "        pat_left = patience\n",
    "    else:\n",
    "        pat_left -= 1\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | val RMSE: {val_rmse:.5f} | val LOGRMSE: {val_rmse_from_norm:.5f} | best: {best_val:.5f} | lr: {lr:.2e} | pat_left: {pat_left}\")\n",
    "\n",
    "    if pat_left == 0:\n",
    "        print(f\"Early stopped at epoch {epoch}, best={best_val:.5f}\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(best_state)\n",
    "best_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52c20eb-4566-46f6-b117-914ae6a8170a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
